{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18694cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: requests in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\otkachuk\\appdata\\local\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec53c376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19375751"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('fhv_tripdata_2019-10.csv.gz', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f0e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   NULL|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   NULL|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   NULL|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('fhv_tripdata') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11be1ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ec2d45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "653b1d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.read_csv('fhv_tripdata_2019-10.csv.gz', nrows=1000)\n",
    "df_schema = spark.createDataFrame(df_pd).schema\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e11714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField ('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField ('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField ('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField ('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField ('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField ('SR_Flag', types.StringType(), True), \n",
    "    types.StructField ('Affiliated_base_number', types.StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')\n",
    "\n",
    "df = df.repartition(6)\n",
    "\n",
    "df.write.parquet('fhv_tripdata/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a83ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with start date on 15th of October: 62610\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "count_on_15th_oct = df.filter(F.dayofmonth(\"pickup_datetime\") == 15).count()\n",
    "\n",
    "print(\"Number of records with start date on 15th of October:\", count_on_15th_oct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0146652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest trip in hours: 631152.5\n"
     ]
    }
   ],
   "source": [
    "df_with_duration = df.withColumn(\n",
    "    \"trip_duration_hours\", \n",
    "    (F.col(\"dropOff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\")) / 3600\n",
    ")\n",
    "\n",
    "max_trip_duration_hours = df_with_duration.agg(F.max(\"trip_duration_hours\")).collect()[0][0]\n",
    "\n",
    "print(\"Length of the longest trip in hours:\", max_trip_duration_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8e23a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12322"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('taxi_zone_lookup.csv', 'wb').write(r.content)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87400883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "\n",
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05c4cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView (\"trips\")\n",
    "df_zones.createOrReplaceTempView (\"tripZones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b371465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "least_trips = spark.sql(\"\"\"\n",
    "    SELECT Zone, COUNT(*) as count\n",
    "    FROM trips\n",
    "    JOIN tripZones ON trips.PUlocationID = tripZones.LocationID\n",
    "    GROUP BY Zone\n",
    "    ORDER BY count ASC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "                        \n",
    "least_trips.show()                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
